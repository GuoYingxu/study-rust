# 推理性能时间分析

## 详细耗时统计

程序现在会显示完整的性能时间分解，帮助你了解每个阶段的耗时：

```
=== 性能统计总结 ===
⏱  模型加载时间: 53.46 ms
⏱  数据准备时间: 0.76 ms
⏱  张量转换时间: 0.02 ms
⏱  纯推理时间:   247.86 ms  ← 核心性能指标
⏱  结果处理时间: 6.64 ms
   ────────────────────────
⏱  总耗时:       532.64 ms

📊 关键性能指标:
   • 推理吞吐量: 4.05 FPS
   • 端到端延迟: 532.64 ms
   • GPU 利用率: 推理前后可见差异
```

## 时间指标说明

### 1. 模型加载时间 (53.46 ms)
- **含义：** 从磁盘加载 ONNX 模型并初始化 GPU 会话的时间
- **优化建议：**
  - 只在程序启动时执行一次，后续推理复用同一个会话
  - 如果需要频繁启动程序，考虑使用模型缓存
- **正常范围：** 50-200 ms（取决于模型大小）

### 2. 数据准备时间 (0.76 ms)
- **含义：** 创建输入数据（512×512 像素）并转换为 ndarray 的时间
- **包含：**
  - 分配内存
  - 填充测试数据（渐变模式）
  - 构建 ndarray 结构
- **正常范围：** < 5 ms
- **优化建议：** 如果处理真实图片，考虑使用内存映射（memmap2）

### 3. 张量转换时间 (0.02 ms)
- **含义：** 将 ndarray 转换为 ONNX Runtime 张量的时间
- **特点：** 非常快，几乎可以忽略不计
- **正常范围：** < 1 ms

### 4. 纯推理时间 (247.86 ms) ⭐ **核心指标**
- **含义：** GPU 实际执行模型推理的时间
- **这是最重要的性能指标！**
- **包含：**
  - 数据传输到 GPU
  - 神经网络计算
  - 结果传输回 CPU
- **影响因素：**
  - GPU 性能（GTX 1050 Ti vs RTX 3090）
  - 输入尺寸（512×512 vs 1024×1024）
  - 批大小（batch size）
  - 模型复杂度

### 5. 结果处理时间 (6.64 ms)
- **含义：** 提取和分析推理结果的时间
- **包含：**
  - 张量数据提取
  - 统计计算（最小值、最大值、平均值）
  - 打印输出
- **正常范围：** 5-20 ms

### 6. 总耗时 (532.64 ms)
- **含义：** 从程序开始到结束的总时间
- **包含：** 所有上述时间 + GPU 信息查询 + 其他开销
- **用途：** 评估端到端性能

## 性能指标解读

### 吞吐量 (Throughput)
```
推理吞吐量: 4.05 FPS (Frames Per Second)
```
- **含义：** 每秒可以处理多少张图片
- **计算方式：** 1000 ms / 纯推理时间
- **应用场景：** 批处理大量图片时的性能指标
- **提升方法：** 增大 batch size（一次处理多张图片）

### 延迟 (Latency)
```
端到端延迟: 532.64 ms
单张图片延迟: 247.86 ms
```
- **端到端延迟：** 完整的处理时间（包括加载、准备、推理、处理）
- **单张延迟：** 仅推理时间
- **应用场景：** 实时应用（如视频处理）关注的指标
- **提升方法：**
  - 使用更快的 GPU
  - 减小输入尺寸
  - 模型量化（FP16/INT8）

## 时间占比分析

基于上述数据：

| 阶段 | 耗时 | 占比 | 说明 |
|-----|------|------|------|
| 模型加载 | 53.46 ms | 10% | 一次性开销 |
| 数据准备 | 0.76 ms | <1% | 可忽略 |
| 张量转换 | 0.02 ms | <1% | 可忽略 |
| **纯推理** | **247.86 ms** | **46%** | **核心瓶颈** |
| 结果处理 | 6.64 ms | 1% | 可优化 |
| 其他开销 | ~224 ms | 42% | GPU 状态查询等 |

**结论：** 纯推理时间是主要瓶颈，优化应集中在此。

## 优化建议

### 场景 1: 批量处理（吞吐量优先）
```rust
let batch_size = 8;  // 一次处理 8 张图片
```
**预期效果：** 吞吐量提升 3-5 倍

### 场景 2: 实时处理（延迟优先）
```rust
let height = 256;  // 减小输入尺寸
let width = 256;
```
**预期效果：** 延迟降低 75%（~60 ms）

### 场景 3: 生产环境优化
1. **复用会话：** 不要每次都重新加载模型
   ```rust
   // 全局或结构体中保持 session
   static SESSION: OnceCell<Session> = OnceCell::new();
   ```

2. **预热 GPU：** 第一次推理通常较慢
   ```rust
   // 运行一次预热推理
   let _ = session.run(dummy_input)?;
   ```

3. **并行处理：** 多线程处理多张图片
   ```rust
   use rayon::prelude::*;
   images.par_iter().for_each(|img| {
       // 推理
   });
   ```

## 性能基准对比

### 不同 GPU 的推理时间（512×512 输入）

| GPU | 纯推理时间 | 吞吐量 | 相对性能 |
|-----|-----------|--------|----------|
| GTX 1050 Ti | ~248 ms | 4.0 FPS | 1x (基准) |
| RTX 3060 | ~80 ms | 12.5 FPS | 3x |
| RTX 3090 | ~40 ms | 25 FPS | 6x |
| RTX 4090 | ~20 ms | 50 FPS | 12x |

### 不同输入尺寸的推理时间（GTX 1050 Ti）

| 尺寸 | 像素数 | 推理时间 | 相对时间 |
|------|--------|----------|----------|
| 256×256 | 65K | ~60 ms | 0.25x |
| 512×512 | 262K | ~248 ms | 1x |
| 1024×1024 | 1M | ~900 ms | 3.6x |

## 监控 GPU 使用情况

### 实时监控（运行推理时）
```bash
# 在另一个终端运行
watch -n 0.5 nvidia-smi
```

### 详细监控
```bash
# 查看 GPU 利用率、显存、温度等
nvidia-smi dmon -i 0 -s pucvmet -d 1
```

### 查看推理时的 GPU 利用率
程序会自动显示推理前后的 GPU 状态：
```
推理前 GPU 状态:
  显存使用: 2147 MB / 4096 MB
  GPU 利用率: 10%
  温度: 42°C

推理后 GPU 状态:
  显存使用: 2147 MB / 4096 MB
  GPU 利用率: 6%
  温度: 42°C
```

## 性能调优检查清单

- [ ] 使用 Release 模式编译（`--release`）
- [ ] 启用 CUDA GPU 加速
- [ ] 根据应用场景选择合适的输入尺寸
- [ ] 复用模型会话，避免重复加载
- [ ] 考虑使用批处理（batch size > 1）
- [ ] 监控 GPU 利用率，确保 GPU 被充分使用
- [ ] 如有多张 GPU，考虑并行推理
- [ ] 评估是否需要模型量化（FP16/INT8）

## 常见性能问题

### Q: 为什么"其他开销"占 42%？
A: 包括 GPU 信息查询（nvidia-smi）、打印输出、统计计算等。在生产环境中，可以移除这些诊断代码。

### Q: 第一次推理特别慢？
A: CUDA 需要初始化和预热。运行一次预热推理可以解决。

### Q: GPU 利用率很低？
A: 可能原因：
1. 输入太小（增大 batch size）
2. 模型太简单（换更大的模型）
3. CPU-GPU 数据传输瓶颈（使用 pinned memory）

### Q: 如何提升 2-3 倍性能？
A: 优先级顺序：
1. 使用更好的 GPU（RTX 系列）✅ 最有效
2. 模型量化为 FP16 ✅ 2倍提升
3. 增大 batch size ✅ 3-5倍吞吐量
4. 优化输入尺寸 ✅ 按需调整

## 总结

**当前性能（GTX 1050 Ti）：**
- 纯推理时间：**247.86 ms**
- 吞吐量：**4.05 FPS**
- GPU 显存使用：**2147 / 4096 MB (52%)**

**优化潜力：**
- 升级到 RTX 3060：**3倍性能提升** → 80 ms / 12 FPS
- 使用 FP16 量化：**2倍性能提升** → 124 ms / 8 FPS
- Batch size = 8：**吞吐量提升至 20-30 FPS**

这些详细的时间统计能帮助你准确定位性能瓶颈，并做出有针对性的优化！
